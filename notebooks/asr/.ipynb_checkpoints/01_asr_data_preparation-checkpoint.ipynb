{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b8e0a85-6a68-41f6-9771-aa19860a2343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f13c23-3c33-427a-90be-b27abec0a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "LANGUAGES = ['en', 'hi']\n",
    "DATA_BASE = '../../data/asr'\n",
    "OUTPUT_BASE = '../../data/asr_processed'\n",
    "AUDIO_DIR = 'train'\n",
    "AUGMENT_HINDI = True\n",
    "HINDI_UPSAMPLE_FACTOR = 9  # To balance Hindi vs. English\n",
    "\n",
    "os.makedirs(OUTPUT_BASE, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94af756e-ba02-4dd1-834b-d747ab2e1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UTILITY FUNCTIONS\n",
    "def list_audio_files(audio_folder):\n",
    "    \"\"\"List all mp3 files in a directory as set.\"\"\"\n",
    "    return set(f for f in os.listdir(audio_folder) if f.endswith('.mp3'))\n",
    "\n",
    "def normalize_text(text, lang):\n",
    "    \"\"\"Basic normalization: lowercase, strip, optionally more.\"\"\"\n",
    "    text = str(text).strip().lower()\n",
    "    return text\n",
    "\n",
    "def filter_tsv(tsv_path, audio_folder, lang):\n",
    "    \"\"\"Return filtered dataframe where audio files exist and text is valid.\"\"\"\n",
    "    df = pd.read_csv(tsv_path, sep='\\t')\n",
    "    # Only mp3s present in subset\n",
    "    audio_files = list_audio_files(audio_folder)\n",
    "    mask_audio = df['path'].apply(lambda x: x in audio_files)\n",
    "    df = df[mask_audio]\n",
    "    # Remove missing/empty text\n",
    "    mask_text = df['sentence'].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)\n",
    "    df = df[mask_text]\n",
    "    # Basic normalizations\n",
    "    tqdm.pandas(desc=f'Normalizing text ({lang})')\n",
    "    df['sentence'] = df['sentence'].progress_apply(lambda x: normalize_text(x, lang))\n",
    "    # Remove duplicates\n",
    "    df = df.drop_duplicates(subset=['sentence', 'path'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def speaker_split(df, val_pct=0.05, test_pct=0.05):\n",
    "    \"\"\"Split dataframe by speaker into train/val/test splits.\"\"\"\n",
    "    speakers = df['client_id'].unique().tolist()\n",
    "    random.shuffle(speakers)\n",
    "    n = len(speakers)\n",
    "    n_val = int(val_pct * n)\n",
    "    n_test = int(test_pct * n)\n",
    "    n_val = max(n_val, 1)\n",
    "    n_test = max(n_test, 1)\n",
    "    val_ids = set(speakers[:n_val])\n",
    "    test_ids = set(speakers[n_val:n_val+n_test])\n",
    "    def get_split(speaker):\n",
    "        if speaker in test_ids:\n",
    "            return 'test'\n",
    "        elif speaker in val_ids:\n",
    "            return 'val'\n",
    "        else:\n",
    "            return 'train'\n",
    "    df['split'] = df['client_id'].apply(get_split)\n",
    "    return df\n",
    "\n",
    "def upsample_hindi(df, factor):\n",
    "    \"\"\"Up-sample Hindi training set to balance with English count.\"\"\"\n",
    "    train_df = df[df['split'] == 'train']\n",
    "    rest_df = df[df['split'] != 'train']\n",
    "    # Resample train set with replacement\n",
    "    upsampled = train_df.sample(n=len(train_df)*factor, replace=True, random_state=42).reset_index(drop=True)\n",
    "    result = pd.concat([upsampled, rest_df], ignore_index=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b260f5a6-4528-419e-87ee-82a5f7e8f709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing language: en\n",
      "Filtering available audio/text pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing text (en): 100%|█████████████████████████| 40000/40000 [00:00<00:00, 1173182.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✂️ Downsampled English dataset to 10000 examples\n",
      "Splitting dataset by speakers...\n",
      "Saved train set: 9004 rows to ../../data/asr_processed/en/train.csv\n",
      "Saved val set: 493 rows to ../../data/asr_processed/en/val.csv\n",
      "Saved test set: 503 rows to ../../data/asr_processed/en/test.csv\n",
      "\n",
      "Processing language: hi\n",
      "Filtering available audio/text pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Normalizing text (hi): 100%|████████████████████████████| 4479/4479 [00:00<00:00, 892502.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset by speakers...\n",
      "Upsampling Hindi train set by factor 9...\n",
      "Saved train set: 18207 rows to ../../data/asr_processed/hi/train.csv\n",
      "Saved val set: 942 rows to ../../data/asr_processed/hi/val.csv\n",
      "Saved test set: 1514 rows to ../../data/asr_processed/hi/test.csv\n"
     ]
    }
   ],
   "source": [
    "# MAIN WORKFLOW\n",
    "processed_stats = []\n",
    "\n",
    "for lang in LANGUAGES:\n",
    "    print(f\"\\nProcessing language: {lang}\")\n",
    "    lang_input_dir = os.path.join(DATA_BASE, lang)\n",
    "    lang_audio_dir = os.path.join(lang_input_dir, AUDIO_DIR)\n",
    "    tsv_path = os.path.join(lang_input_dir, 'train.tsv')\n",
    "    \n",
    "    print(\"Filtering available audio/text pairs...\")\n",
    "    df = filter_tsv(tsv_path, lang_audio_dir, lang)\n",
    "    \n",
    "    # Limit English data to 10,000 rows (after filtering)\n",
    "    if lang == 'en':\n",
    "        df = df.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "        print(f\"✂️ Downsampled English dataset to {len(df)} examples\")\n",
    "    \n",
    "    print(\"Splitting dataset by speakers...\")\n",
    "    df = speaker_split(df)\n",
    "    \n",
    "    # Save initial stats\n",
    "    counts = df['split'].value_counts().to_dict()\n",
    "    counts['lang'] = lang\n",
    "    counts['total_clips'] = len(df)\n",
    "    processed_stats.append(counts)\n",
    "\n",
    "    # Upsample Hindi training set to balance it\n",
    "    if lang == 'hi' and AUGMENT_HINDI:\n",
    "        print(f\"Upsampling Hindi train set by factor {HINDI_UPSAMPLE_FACTOR}...\")\n",
    "        df = upsample_hindi(df, HINDI_UPSAMPLE_FACTOR)\n",
    "    \n",
    "    # Save metadata for each split\n",
    "    outdir = os.path.join(OUTPUT_BASE, lang)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        split_df = df[df['split'] == split]\n",
    "        outfile = os.path.join(outdir, f'{split}.csv')\n",
    "        split_df.to_csv(outfile, index=False)\n",
    "        print(f\"Saved {split} set: {len(split_df)} rows to {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f842a14f-b844-4e43-8f58-ae2e184ae28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final dataset statistics (after processing):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>val</th>\n",
       "      <th>lang</th>\n",
       "      <th>total_clips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9004</td>\n",
       "      <td>503</td>\n",
       "      <td>493</td>\n",
       "      <td>en</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>1514</td>\n",
       "      <td>942</td>\n",
       "      <td>hi</td>\n",
       "      <td>4479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train  test  val lang  total_clips\n",
       "0   9004   503  493   en        10000\n",
       "1   2023  1514  942   hi         4479"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data preparation and split complete!\n"
     ]
    }
   ],
   "source": [
    "# SUMMARY STATUS\n",
    "print(\"\\nFinal dataset statistics (after processing):\")\n",
    "final_stats = pd.DataFrame(processed_stats)\n",
    "display(final_stats)\n",
    "\n",
    "# Save overview stats\n",
    "final_stats.to_csv(os.path.join(OUTPUT_BASE, 'asr_data_stats.csv'), index=False)\n",
    "\n",
    "print(\"\\nData preparation and split complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
